# ðŸ“šDo-GOOD: Towards Distribution Shift Evaluation for Pre-Trained Visual Document Understanding Models

# Table of contents
* [Overview](#overview)
* [Requirement](#requirement)
* [Installation](#installation)
* [Datasets](#datasets)
* [Tuning and Testing](#tuning-and-testing)
* [Results](#results)

# Overview

The Do-GOOD warehouse is the analysis for document changes in the three modal distributions of image, layout and text. It covers the generation of nine kinds of OOD data, the application of five shifts, the acquisition of FUNSD-H and FUNSD-R datasets, the generation of FUNSD-L datasets, and the running of two kinds of OOD baseline methods Deep Core and Mixup codes under all shift.

The shift type of the Do GOOD dataset is shown in the following figure.

![](https://user-images.githubusercontent.com/111342294/216049177-b6b1c993-bdd4-4716-9089-d5e1ce84deec.png)



# Requirement

This code is developed with

```javascript
transformers              4.24.0 
pytesseract               0.3.9 
tesseract                 0.1.3     
textattack                0.3.7 
python                    3.9.11
yarl                      1.7.2
detectron2                0.6                         
editdistance              0.6.0                    
einops                    0.4.1
```

# Installation

Installation for Projectï¼Œif you need to study the robustness of the model to text shift, you need to install [Textattack](https://github.com/QData/TextAttack)

```
git clone https://anonymous.4open.science/r/Do-GOOD-D88A && cd Do-GOOD
```

# Datasets

We provide manually labeled FUNSD-H and FUNSD-R, which can be obtained from the links below, and methods for generating FUNSD-L, CDIP-L, CDIP-I<sub>1</sub> and CDIP-I<sub>2</sub> datasets.

| Dataset | Header      | Question      | Answer      | Other      | Total      | Link      |
|:--------:| :------------:| :------------:| :------------:| :------------:|:------------:|:------------:|
| FUNSD | 122 | 1077 | 821 | 312 | 2332 |[download](https://pan.baidu.com/s/18OHBdaJCtFWTovHulJGAiQ)|
| FUNSD-H | 126 | 981 | 755 | 380 | 2304 |[download](https://pan.baidu.com/s/15L3Kyc2-NcpXqb6o7cd-HQ)|
| FUNSD-R | 90 | 475 | 445 | 471 | 1487 |[download](https://pan.baidu.com/s/1yrm0YANgX290ZMhpTBi8Cg)|


### Generate FUNSD-L

First generate strong and weak semantic entities and get the following files , `/weak_other_map` , `/strong_answer_map` , `/strong_question_map` , `/weak_Q_map`   , `/weak_A_map`,We provide five strong and weak semantic entity libraries extracted from our shuffle layout method on the FUNSD test set for five different pre-training models ,You can choose to fill in `v3`, `v2`, `v1`, `bros` or `lilt` in { } and execute the following code
```
python map_{ }_funsd_L.py
```

Then modify the file path to generate FUNSD-L test data , which is saved in the `mix_test.txt` , you can modify the number of rows and columns generated by the layout, the size of the bounding box, the probability of random filling, and the number of documents generated

```
generate_ood_data("mix_test.txt", "/strong_question_map",
                  "/strong_answer_map", "/weak_Q_map",
                  "/weak_A_map", "/weak_other_map",50)
```

```
python gen_ood_mix.py
```

![](https://user-images.githubusercontent.com/111342294/202719602-47a09c21-0226-4221-9652-6d714b4a4a46.png)


### Generate CDIP-L

To facilitate use, we separately place it in the main directory, and adjust two parameters: lamda1 controls the horizontal distance, and lamda2 controls the vertical distance. We use the priority order of consolidation: horizontal first and then vertical

```
python merge_layout.py
```

![](https://user-images.githubusercontent.com/111342294/202724209-b915d944-dd62-4e77-a66e-781bc4b4a707.png)


### Generate CDIP-I<sub>1</sub>

Separate text pixels and non text pixels in the document, and then overlay them into the natural scene [MSCOCO](https://cocodataset.org/#home)

```
python python mixup_image.py
```

![](https://user-images.githubusercontent.com/111342294/202724449-f8ee8ffd-c8aa-4dd7-b665-1a6558b5e7aa.png)

### Generate CDIP-I<sub>2</sub>

Using pre-trained [DocGeoNet](https://github.com/fh2019ustc/DocGeoNet)(specific process reference), a forward propagation calculation of the normal document image is performed to get the distorted image, and then OCR again

```
python inference.py
```

![](https://user-images.githubusercontent.com/111342294/202726278-e0a89790-494e-46a6-8a2e-42009f2dfce4.png)


# Tuning and Testing

### Tuning

Select the model used and the task fill, the first { } select `v3`, `v2`, `v1`, `bros` or `lilt`, the second {} select`funsd` or `cdip`

Finetune your own LayoutLMv3 model or download our finetuned model [download](https://pan.baidu.com/s/1zwlTvQsJfQDVOo2UDMRgRA)ï¼ŒSelect models and tasks to use

```javascript
python -m torch.distributed.launch --nproc_per_node --use_env finetune_{ }_{ }.py --config config.yaml --output_dir
```

For VQA tasks, use the command line aloneï¼Œfill in the selected model at { }

```javascript
python docvqa_{}_main.py
```

### Testing

Select the model used and the task fill, the first { } select `v3`, `v2`, `v1`, `bros` or `lilt`, the second { } select`funsd` or `cdip` ï¼Œ modify the following parameters to perform a shift operation on a mode.`--text_aug`,`--image_aug`,`--aut_layout`
```
--text_aug={'WordSwapMaskedLM','WordSwapEmbedding','WordSwapHomoglyphSwap','WordSwapChangeNumber','WordSwapRandomCharacterDeletion'} , --image_aug=True/False , --aug_layout=True/False
```

```
python demo_{ }_ood_{ }.py
```

# Results

### The ID and OOD performance of the existing models
![image](https://user-images.githubusercontent.com/111342294/202836388-7e251a9c-ad73-4d16-bfc3-a0d6ba11f9dc.png)

### Incremental training results on the FUNSD and CDIP datasets
![](https://user-images.githubusercontent.com/111342294/216049813-ef25379f-317f-4bf8-809e-bbc8979085ea.png)

# Algorithm details
![](https://user-images.githubusercontent.com/111342294/216042901-f8e00e98-1e4a-4f62-a042-fb2831ebe650.png)
![](https://user-images.githubusercontent.com/111342294/216049472-3f539605-ca00-492a-b255-45c1638b269e.png)


# Visualize the results
![](https://user-images.githubusercontent.com/111342294/216043281-e05d76ac-25bc-4d2e-9973-c64e3b5bf1ff.png)
![](https://user-images.githubusercontent.com/111342294/216045562-c7131d37-390c-4a70-aa6b-de85b0b4f280.png)
![](https://user-images.githubusercontent.com/111342294/216045726-838a230d-56b7-4884-994d-ba86f93fdb9a.png)
![](https://user-images.githubusercontent.com/111342294/216048964-eca2e4ec-5c0d-45d5-b183-15c3d5d94108.png)
![](https://user-images.githubusercontent.com/111342294/216049716-3bb74c66-47b2-406d-adf7-de6c6e7b4d6a.png)
